---
title: 'Individual Project # 2'
author: "Julia Barnhart"
date: "February 25, 2018"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

1. Download the classification output data set (attached in Canvas to the assignment).

```{r message=FALSE, warning=FALSE, results=TRUE}

data <- read.csv("C:/Users/Julia/Desktop/classification-output-data.csv", sep=',', header = T)
attach(data)

```

2. The data set has three key columns we will use:
    . class: the actual class for the observation
    . scored.class: the predicted class for the observation (based on a threshold of 0.5)
    . scored.probability: the predicted probability of success for the observation
Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand the output. In particular, do the rows represent the actual or predicted class? The columns?

```{r message=FALSE, warning=FALSE, results=TRUE}

table(class,scored.class)
head(data)

```

3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.

```{r message=FALSE, warning=FALSE, results=TRUE}

pred_accuracy <- function(arg1){
  f_table = table(arg1$class, arg1$scored.class)
  accuracy = round((f_table[2,2] + f_table[1,1]) / (f_table[2,2] + f_table[1,2] + f_table[1,1] + f_table[2,1]), 4)
  return(accuracy)
}

pred_accuracy(data)

```

4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.

```{r message=FALSE, warning=FALSE, results=TRUE}

class_error_rate <- function(arg1){
  f_table = table(arg1$class, arg1$scored.class)
  error_rate = round((f_table[1,2] + f_table[2,1]) / (f_table[2,2] + f_table[1,2] + f_table[1,1] + f_table[2,1]), 4)
  return(error_rate)
}

class_error_rate(data)

```

5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

```{r message=FALSE, warning=FALSE, results=TRUE}

pred_precision <- function(arg1){
  f_table = table(arg1$class, arg1$scored.class)
  precision = round(f_table[2,2] / (f_table[2,2] + f_table[1,2]), 4)
  return(precision)
}

pred_precision(data)

```

6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

```{r message=FALSE, warning=FALSE, results=TRUE}

pred_sensitivity <- function(arg1){
  f_table = table(arg1$class, arg1$scored.class)
  sensitivity = round(f_table[2,2] / (f_table[2,2] + f_table[2,1]), 4)
  return(sensitivity)
}

pred_sensitivity(data)

```

7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

```{r message=FALSE, warning=FALSE, results=TRUE}

pred_specificity <- function(arg1){
  f_table = table(arg1$class, arg1$scored.class)
  specificity = round(f_table[1,1] / (f_table[1,1] + f_table[1,2]), 4)
  return(specificity)
}

pred_specificity(data)

```

8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.

```{r message=FALSE, warning=FALSE, results=TRUE}

f1_score <- function(arg1){
  f_table = table(arg1$class, arg1$scored.class)
  precision = round(f_table[2,2] / (f_table[2,2] + f_table[1,2]), 4)
  sensitivity = round(f_table[2,2] / (f_table[2,2] + f_table[2,1]), 4)
  f1 = (2 * (precision * sensitivity)) / (precision + sensitivity)
  return(f1)
}

f1_score(data)

```

9. Let's consider the following question: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. 

    The bounds of precision are as follows:
    0 <= precision <= 1

    The bounds of sensitivity are as follows:
    0 <= sensitivity <= 1
  
    Therefore,

    0 <= precision x sensitivity <= 1 x sensitivity (multiply both sides by sensitivtiy)

    0 <= 2 x precision x sensitivity <= 2 x sensitivity (multiply both sides by 2) (Eq.1)

    In addition,

    0 <= precision + sensitivity <= 2 (Eq.2)

    Ergo,

    0 <= (2 x precision x sensitivity)/(precision + sensitivity) <= (2 x sensitivity)/2 (divide Eq. 1 by Eq. 2)

    0 <= (2 x precision x sensitivity)/(precision + sensitivity) <= sensitivity

    0 <= F1 <= sensitivity

    As sensitivity is bounded [0,1], F1 is bounded by [0,1]

10. Write a function that generates an ROC curve from a data set with a true classification column (i.e., class) and a probability column (i.e., scored.probability). Your function should return the plot of the ROC curve and the calculated area under the ROC curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.

```{r message=FALSE, warning=FALSE, results=TRUE}

library(caret)

rocplot <- function(arg1){
  
  d_workhorse = arg1[,c("class", "scored.probability")] # extract needed columns from dataframe
  
  v_thresh <- seq(0.01, 1, by = .01) # vector of threshold values beteen 0 and 1 by .01
  
  v_sensitivity <- rep(0, length(v_thresh)) # vector to hold sensitivity
  v_specificity <- rep(0, length(v_thresh)) # vector to hold specificity
  
  
  for(x in 1:100) {
    
    pred_class <- rep(0, nrow(d_workhorse)) # vector to store predicted class in each threshold iteration
    
    pred_class[d_workhorse$scored.probability > v_thresh[x]] <- 1 # compare against threshold value
    
    # using built-in functions from caret package
    v_sensitivity[x] = sensitivity(factor(pred_class), factor(d_workhorse$class))
    v_specificity[x] = (1 - specificity(factor(pred_class), factor(d_workhorse$class)))
    
  }

  # plot ROC curve 
  
  roc = data.frame(cbind(v_sensitivity, v_specificity))

  plot_roc = ggplot(roc, aes(x=roc$v_specificity, y=roc$v_sensitivity)) +
    geom_line(color=rgb(0,0,1,alpha=0.3)) + 
    labs(title = "Receiver Operating Curve") + xlab("1-Specificity") +
    ylab("Sensitivity") +
    geom_abline(aes(color="BR", slope=1, intercept=0)) +
    guides(colour=FALSE) +
    scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

  # calcualte area under curve (AUC) using simple approximation method

  # dataframe with spaces between values
  dspec = c(diff(v_specificity), 0)
  dsens = c(diff(v_sensitivity), 0)

  auc = sum(v_sensitivity * dspec) + sum(dsens * dspec) / 2 

  results = list(plot_roc, auc) # return a list of results

  return(results)

}

```

11. Use your created R functions and the provided classification output data set to produce all of the classification metrics discussed above.

```{r message=FALSE, warning=FALSE, results=TRUE}

pred_accuracy(data)
class_error_rate(data)
pred_precision(data)
pred_sensitivity(data)
pred_specificity(data)
f1_score(data)
rocplot(data)

```

12. Investigate the caret package. In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions?

In general, the function confusionMatrix() provides a great array of metrics. This includes both sensitivity and specificity. According to the results, these values match the results of the pred_sensitivity() and pred_specificity() function defined above. However, one difference is that by default the caret package treats zeroes as positive outcomes and ones as negative outcomes. 

```{r message=FALSE, warning=FALSE, results=TRUE}

# My confusion matrix
table(class,scored.class)

# caret
confusionMatrix(factor(scored.class), factor(class))

```

Investigate the pROC package. Use it to generate an ROC curve for the data set. How do the results compare with your own functions?

One of the differences between the ROC curve produced by the user-defined function rocplot() above and the one by the pROC package is that the former's x-axis is delinieated by "1-specificity" rather than just "specificity", which flips the axis over and changes the visual interpretation. In addition, the appearance of the curve is different as well. This is because the pROC packages uses the trapezoidal rule for approximating AUC rather than the rectangular rule used in rocplot(). Lastly, the values for AUC are slightly different, as expected. The roc() function from pROC gives 0.8503, while the rocplot() function gives 0.8489. 

```{r message=FALSE, warning=FALSE, results=TRUE}

library(pROC)

# pROC version
pproc=roc(class~scored.probability, data)
plot(pproc)
pproc$auc

```


